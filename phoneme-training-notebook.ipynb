{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61954,"databundleVersionId":6762309,"sourceType":"competition"},{"sourceId":6833907,"sourceType":"datasetVersion","datasetId":3928977}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nrandom.seed(57)","metadata":{"id":"bGyd11R9Gz0G","execution":{"iopub.status.busy":"2023-10-30T09:20:35.517548Z","iopub.execute_input":"2023-10-30T09:20:35.518152Z","iopub.status.idle":"2023-10-30T09:20:49.708895Z","shell.execute_reply.started":"2023-10-30T09:20:35.518117Z","shell.execute_reply":"2023-10-30T09:20:49.707706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\npath = \"/kaggle/input/text2ipa-mapping-trainset/previous_trainset_word_ipa_map_37807.csv\"\ndf = pd.read_csv(path).drop('Unnamed: 0', axis=1)\nprint(\"Length of training set\", len(df))\ndf.head(5)","metadata":{"id":"pyFtcaV3Gz0H","outputId":"3414c36c-9c42-42ba-f5e2-84344a4d9442","execution":{"iopub.status.busy":"2023-10-30T09:21:51.044989Z","iopub.execute_input":"2023-10-30T09:21:51.045461Z","iopub.status.idle":"2023-10-30T09:21:51.131491Z","shell.execute_reply.started":"2023-10-30T09:21:51.045428Z","shell.execute_reply":"2023-10-30T09:21:51.130496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notebook for creating word - ipa mapping: \n\nhttps://www.kaggle.com/code/jakir57/dataverse-eda-dictionary-creation","metadata":{}},{"cell_type":"markdown","source":"# Converting words into sentences of characters","metadata":{}},{"cell_type":"code","source":"def word_sentence(word):\n  sentence = \"\"\n  for ch in word:\n    sentence += (ch + \" \")\n  return sentence\n\ntext_pairs = []\nfor index, row in df.iterrows():\n  word = word_sentence(row[\"word\"])\n  ipa = row[\"ipa\"]\n  ipa = \"[start] \" + word_sentence(ipa) + \"[end]\"\n  text_pairs.append((word, ipa))\nprint(len(text_pairs), \"training pairs loaded..\")","metadata":{"id":"kQeujujzGz0I","outputId":"bbeb27aa-cb1a-4b1b-98ba-ea6b460b7bbb","execution":{"iopub.status.busy":"2023-10-30T09:24:51.855728Z","iopub.execute_input":"2023-10-30T09:24:51.856090Z","iopub.status.idle":"2023-10-30T09:24:54.476084Z","shell.execute_reply.started":"2023-10-30T09:24:51.856060Z","shell.execute_reply":"2023-10-30T09:24:54.475125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample training pairs","metadata":{}},{"cell_type":"code","source":"for _ in range(5):\n    print(random.choice(text_pairs))","metadata":{"id":"1kelDUkrGz0J","outputId":"8772d7b6-e196-4a62-cbe0-1b7b6a5ef203","execution":{"iopub.status.busy":"2023-10-30T09:25:27.790405Z","iopub.execute_input":"2023-10-30T09:25:27.790769Z","iopub.status.idle":"2023-10-30T09:25:27.795862Z","shell.execute_reply.started":"2023-10-30T09:25:27.790740Z","shell.execute_reply":"2023-10-30T09:25:27.794958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(text_pairs)\nnum_val_samples = int(0.01 * len(text_pairs))\nnum_train_samples = len(text_pairs) -  num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples :]\n\nprint(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")","metadata":{"id":"H2HFz4B3Gz0J","outputId":"bed71273-a379-47ae-a6b5-57cc495fee8e","execution":{"iopub.status.busy":"2023-10-30T09:26:38.929336Z","iopub.execute_input":"2023-10-30T09:26:38.929696Z","iopub.status.idle":"2023-10-30T09:26:38.971149Z","shell.execute_reply.started":"2023-10-30T09:26:38.929668Z","shell.execute_reply":"2023-10-30T09:26:38.970172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vocabulary creation and Text to vectorization","metadata":{}},{"cell_type":"code","source":"sequence_length = 64\nbatch_size = 64\n\n# Defining vocabulary\nvb = ['', '[UNK]', '[start]', '[end]', 'া', 'র', '্', 'ে', 'ি', 'ন', 'ক', 'ব', 'স', 'ল', 'ত', 'ম', 'প', 'ু', 'দ', 'ট', 'য়', 'জ', '।', 'ো', 'গ', 'হ', 'য', 'শ', 'ী', 'ই', 'চ', 'ভ', 'আ', 'ও', 'ছ', 'ষ', 'ড', 'ফ', 'অ', 'ধ', 'খ', 'ড়', 'উ', 'ণ', 'এ', 'থ', 'ং', 'ঁ', 'ূ', 'ৃ', 'ঠ', 'ঘ', 'ঞ', 'ঙ', 'ৌ', '‘', 'ৎ', 'ঝ', 'ৈ', '়', 'ঢ', 'ঃ', 'ঈ', '\\u200c', 'ৗ', 'a', 'ঐ', 'd', 'w', 'ঋ', 'i', 'e', 't', 's', 'n', 'm', 'b', '“', 'u', 'r', 'œ', 'o', '–', 'ঊ', 'ঢ়', 'Í', 'g', 'p', '\\xad', 'h', 'c', 'l', 'ঔ', 'ƒ', '”', 'Ñ', '¡', 'y', 'j', 'f', '→', '—', 'ø', 'è', '¦', '¥', 'x', 'v', 'k']\nvipa = ['', '[UNK]', '[start]', '[end]', 'ɐ', 'ɾ', 'i', 'o', 'e', '̪', 't', 'n', 'k', 'ɔ', 'ʃ', 'b', 'd', 'l', 'u', 'p', 'm', 'ʰ', 'ɟ', '͡', '̯', 'g', 'ʱ', '।', 'c', 'ʲ', 'h', 's', 'ŋ', 'ɛ', 'ɽ', '̃', 'ʷ', '‘', '“', '–', '”', '—', 'w', 'j']\n\nv = vb + vipa\ns = set()\nfor ch in v:\n  s.add(ch)\n\nvocab = sorted(list(s))\nprint(\"Size of vocabulary is\", len(s))\nprint(vocab)\nvocab_size = len(vocab)\n\n\neng_vectorization = TextVectorization(\n    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n    vocabulary=vocab\n)\nspa_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    vocabulary=vocab\n)\ntrain_eng_texts = [pair[0] for pair in text_pairs]\ntrain_spa_texts = [pair[1] for pair in text_pairs]\n\n# eng_vectorization.adapt(train_eng_texts)\n# spa_vectorization.adapt(train_spa_texts)","metadata":{"id":"adChEXp5Gz0K","outputId":"fd55a230-78ed-4a15-cc15-2f282e01a3a0","execution":{"iopub.status.busy":"2023-10-30T09:31:23.629633Z","iopub.execute_input":"2023-10-30T09:31:23.629995Z","iopub.status.idle":"2023-10-30T09:31:23.671006Z","shell.execute_reply.started":"2023-10-30T09:31:23.629966Z","shell.execute_reply":"2023-10-30T09:31:23.670013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating vectorized train and validation set","metadata":{}},{"cell_type":"code","source":"def format_dataset(eng, spa):\n    eng = eng_vectorization(eng)\n    spa = spa_vectorization(spa)\n    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n\ndef make_dataset(pairs):\n    eng_texts, spa_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    spa_texts = list(spa_texts)\n    print(spa_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)","metadata":{"id":"0UpCtmt_Gz1M","outputId":"d93e5dbf-de62-47fe-e867-7ffb66ed63eb","execution":{"iopub.status.busy":"2023-10-30T09:33:36.364266Z","iopub.execute_input":"2023-10-30T09:33:36.365111Z","iopub.status.idle":"2023-10-30T09:33:36.955504Z","shell.execute_reply.started":"2023-10-30T09:33:36.365077Z","shell.execute_reply":"2023-10-30T09:33:36.954478Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in train_ds.take(1):\n    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n    print(f\"targets.shape: {targets.shape}\")","metadata":{"id":"wxTAW1_pGz1N","outputId":"de42c896-6ae2-4579-a21a-b5316b872daf","execution":{"iopub.status.busy":"2023-10-30T09:33:49.309979Z","iopub.execute_input":"2023-10-30T09:33:49.310699Z","iopub.status.idle":"2023-10-30T09:33:49.873682Z","shell.execute_reply.started":"2023-10-30T09:33:49.310666Z","shell.execute_reply":"2023-10-30T09:33:49.872771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer model","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n        attention_output = self.attention(\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"dense_dim\": self.dense_dim,\n            \"num_heads\": self.num_heads,\n        })\n        return config\n\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"sequence_length\": self.sequence_length,\n            \"vocab_size\": self.vocab_size,\n            \"embed_dim\": self.embed_dim,\n        })\n        return config\n\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        proj_output = self.dense_proj(out_2)\n        return self.layernorm_3(out_2 + proj_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"latent_dim\": self.latent_dim,\n            \"num_heads\": self.num_heads,\n        })\n        return config\n","metadata":{"id":"TVriAVGhGz1P","execution":{"iopub.status.busy":"2023-10-30T09:34:21.019477Z","iopub.execute_input":"2023-10-30T09:34:21.020128Z","iopub.status.idle":"2023-10-30T09:34:21.043152Z","shell.execute_reply.started":"2023-10-30T09:34:21.020094Z","shell.execute_reply":"2023-10-30T09:34:21.042408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 256\nlatent_dim = 2048\nnum_heads = 8\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\nencoder = keras.Model(encoder_inputs, encoder_outputs)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\nencoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\nx = layers.Dropout(0.5)(x)\ndecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\ndecoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\ntransformer = keras.Model(\n    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n)","metadata":{"id":"HVwtFdXsGz1Q","execution":{"iopub.status.busy":"2023-10-30T09:34:35.799280Z","iopub.execute_input":"2023-10-30T09:34:35.799631Z","iopub.status.idle":"2023-10-30T09:34:36.949968Z","shell.execute_reply.started":"2023-10-30T09:34:35.799602Z","shell.execute_reply":"2023-10-30T09:34:36.949207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"epochs =  50 # This should be at least 30 for convergence\n\ntransformer.summary()\ntransformer.compile(\n    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\nhistory=transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)","metadata":{"id":"BDyDAFFzGz1R","outputId":"058162e4-e8e3-4040-ede3-4d21a23a19cd","execution":{"iopub.status.busy":"2023-10-30T09:35:01.903553Z","iopub.execute_input":"2023-10-30T09:35:01.903946Z","iopub.status.idle":"2023-10-30T10:21:01.737042Z","shell.execute_reply.started":"2023-10-30T09:35:01.903915Z","shell.execute_reply":"2023-10-30T10:21:01.736209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy and loss plot","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title(\"Model Accuaracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\",\"valiadtion\"],loc='upper left')\nplt.show()","metadata":{"id":"wJeS-mr4Gz1S","outputId":"7f7ca30e-69e7-4697-ccf9-70a046c2cd95","execution":{"iopub.status.busy":"2023-10-30T10:24:06.880103Z","iopub.execute_input":"2023-10-30T10:24:06.880977Z","iopub.status.idle":"2023-10-30T10:24:07.290962Z","shell.execute_reply.started":"2023-10-30T10:24:06.880936Z","shell.execute_reply":"2023-10-30T10:24:07.290004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"epoch\")\nplt.legend([\"training loss\",\"validation loss\"],loc='upper left')\nplt.show()","metadata":{"id":"3-Er5e2oGz1T","outputId":"8018f4a7-03e5-4e23-c881-6873ae2c841b","execution":{"iopub.status.busy":"2023-10-30T10:24:11.390972Z","iopub.execute_input":"2023-10-30T10:24:11.391326Z","iopub.status.idle":"2023-10-30T10:24:11.694166Z","shell.execute_reply.started":"2023-10-30T10:24:11.391297Z","shell.execute_reply":"2023-10-30T10:24:11.693227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the model","metadata":{}},{"cell_type":"code","source":"tf.saved_model.save(transformer, export_dir='/kaggle/working/text2ipa-transformer-model')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T10:30:29.364421Z","iopub.execute_input":"2023-10-30T10:30:29.365378Z","iopub.status.idle":"2023-10-30T10:30:38.507982Z","shell.execute_reply.started":"2023-10-30T10:30:29.365335Z","shell.execute_reply":"2023-10-30T10:30:38.507035Z"},"trusted":true},"execution_count":null,"outputs":[]}]}