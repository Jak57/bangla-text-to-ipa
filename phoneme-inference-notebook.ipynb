{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61954,"databundleVersionId":6762309,"sourceType":"competition"},{"sourceId":6833907,"sourceType":"datasetVersion","datasetId":3928977},{"sourceId":6835207,"sourceType":"datasetVersion","datasetId":3929755}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries","metadata":{"id":"a-Nb01r0LXTB"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:01:57.084685Z","iopub.execute_input":"2023-10-30T14:01:57.085054Z","iopub.status.idle":"2023-10-30T14:01:57.089949Z","shell.execute_reply.started":"2023-10-30T14:01:57.085024Z","shell.execute_reply":"2023-10-30T14:01:57.088918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading model and setting up vocabulary","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/bangla-text2ipa-transformer-model/text2ipa-transformer-model'\nnew_model=tf.saved_model.load(path)\n\nvb = ['', '[UNK]', '[start]', '[end]', 'া', 'র', '্', 'ে', 'ি', 'ন', 'ক', 'ব', 'স', 'ল', 'ত', 'ম', 'প', 'ু', 'দ', 'ট', 'য়', 'জ', '।', 'ো', 'গ', 'হ', 'য', 'শ', 'ী', 'ই', 'চ', 'ভ', 'আ', 'ও', 'ছ', 'ষ', 'ড', 'ফ', 'অ', 'ধ', 'খ', 'ড়', 'উ', 'ণ', 'এ', 'থ', 'ং', 'ঁ', 'ূ', 'ৃ', 'ঠ', 'ঘ', 'ঞ', 'ঙ', 'ৌ', '‘', 'ৎ', 'ঝ', 'ৈ', '়', 'ঢ', 'ঃ', 'ঈ', '\\u200c', 'ৗ', 'a', 'ঐ', 'd', 'w', 'ঋ', 'i', 'e', 't', 's', 'n', 'm', 'b', '“', 'u', 'r', 'œ', 'o', '–', 'ঊ', 'ঢ়', 'Í', 'g', 'p', '\\xad', 'h', 'c', 'l', 'ঔ', 'ƒ', '”', 'Ñ', '¡', 'y', 'j', 'f', '→', '—', 'ø', 'è', '¦', '¥', 'x', 'v', 'k']\nvipa = ['', '[UNK]', '[start]', '[end]', 'ɐ', 'ɾ', 'i', 'o', 'e', '̪', 't', 'n', 'k', 'ɔ', 'ʃ', 'b', 'd', 'l', 'u', 'p', 'm', 'ʰ', 'ɟ', '͡', '̯', 'g', 'ʱ', '।', 'c', 'ʲ', 'h', 's', 'ŋ', 'ɛ', 'ɽ', '̃', 'ʷ', '‘', '“', '–', '”', '—', 'w', 'j']\nv = vb + vipa\ns = set()\nfor ch in v:\n  s.add(ch)\n\nvocab = sorted(list(s))\nprint(\"Length of vocab:\", len(s))\nprint(vocab)\nvocab_size = len(vocab)\n\nsequence_length = 64 # 20\nbatch_size = 64\n\neng_vectorization = TextVectorization(\n    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n    vocabulary=vocab\n)\n\nspa_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    vocabulary=vocab\n)\n\nspa_vocab = spa_vectorization.get_vocabulary()\nspa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\nmax_decoded_sentence_length = 64 #20\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = eng_vectorization([input_sentence])\n    decoded_sentence = '[start]'\n\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n        predictions = new_model([tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = spa_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == '[UNK]':\n            break\n    return decoded_sentence\n\ndef sentence_word(sentence):\n  trg=''\n  for ch in sentence:\n      if ch != \" \":\n        trg += ch\n  return trg\n\ndef word_sentence(word):\n  sentence = \"\"\n  for ch in word:\n    sentence += (ch + \" \")\n  return sentence\n","metadata":{"id":"CO4-b5G-Gz1V","outputId":"81a2ee5e-269c-422d-c2d7-c9e3c27b9484","execution":{"iopub.status.busy":"2023-10-30T12:29:47.062071Z","iopub.execute_input":"2023-10-30T12:29:47.062812Z","iopub.status.idle":"2023-10-30T12:29:55.178525Z","shell.execute_reply.started":"2023-10-30T12:29:47.062778Z","shell.execute_reply":"2023-10-30T12:29:55.177486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing steps","metadata":{"id":"5ARtfO3XGz1W"}},{"cell_type":"code","source":"def bangla_vocabulary():\n  Vowels = ['অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ঌ', 'এ', 'ঐ', 'ও', 'ঔ']\n  Vowel_signs = ['া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ৄ', 'ে', 'ৈ', 'ো', 'ৌ']\n  Consonants = ['ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', 'ড়', 'ঢ়', 'য়', 'ৎ', 'ং', 'ঃ', 'ঁ']\n  Operators = ['=', '+', '-', '*', '/', '%', '<', '>', '×', '÷']\n  Punctuation_marks = ['।', ',', ';', ':', '?', '!', \"'\", '.', '\"', '-', '[', ']', '{', '}', '(', ')', '–', '—', '―', '~']\n  Others = ['্', '়', 'ৗ', '‘', '’', '“', '”']\n\n  BANGLA_VOCAB = sorted(list(set(Vowels + Vowel_signs + Consonants +  Operators + Punctuation_marks + Others)))\n  return BANGLA_VOCAB\n\ndef foreign_character_normalization(word):\n  BANGLA_VOCAB = bangla_vocabulary()\n  normalized_word = \"\"\n\n  for ch in word:\n    if ch not in BANGLA_VOCAB:\n      continue\n    normalized_word += ch\n  return normalized_word\n\ndef aligned_stateful_tokenizer(word):\n  vocab = [ 'ঁ', 'ং', 'ঃ', 'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'এ', 'ঐ', 'ও', 'ঔ', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', '়', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ', '্', 'ৎ', 'ৗ', 'ড়', 'ঢ়', 'য়']\n  n = len(word)\n  i = 0\n  j = n-1\n\n  state = []\n  tokens = []\n\n  while i < n:\n    subword = \"\"\n    if word[i] in vocab:\n      found = True\n      while i < n and word[i] in vocab:\n        subword += word[i]\n        i += 1\n\n    elif not(word[i] in vocab):\n      found = False\n      while i < n and not(word[i] in vocab):\n        subword += word[i]\n        i += 1\n\n    state.append(found)\n    tokens.append(subword)\n  return state, tokens\n\ndef preprocess(word):\n  preprocessed_word = foreign_character_normalization(word)\n  return preprocessed_word","metadata":{"id":"Sev_weRUGz1W","execution":{"iopub.status.busy":"2023-10-30T12:30:06.471159Z","iopub.execute_input":"2023-10-30T12:30:06.471540Z","iopub.status.idle":"2023-10-30T12:30:06.486730Z","shell.execute_reply.started":"2023-10-30T12:30:06.471509Z","shell.execute_reply":"2023-10-30T12:30:06.485784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Submission File for Competition","metadata":{"id":"BA4rZ9dDGz1Y"}},{"cell_type":"markdown","source":"# Loading Dictionary from Training set","metadata":{"id":"NJl8tpVlGz1Z"}},{"cell_type":"code","source":"path = \"/kaggle/input/text2ipa-mapping-trainset/previous_trainset_word_ipa_map_37807.csv\"\ndf = pd.read_csv(path)\n\nDICTIONARY = {}\nvocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\ndigits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nENGLISH_VOCAB = vocab + digits\n\nproblem = []\nfor index, row in df.iterrows():\n  word = row['word']\n  ipa = row['ipa']\n  DICTIONARY[word] = ipa\n\n# correcting incorrect annotaions\nDICTIONARY[\"seen\"] = \"\"\nDICTIONARY[\"passage\"] = \"\"\nDICTIONARY[\"Writing\"] = \"\"\nDICTIONARY[\"Test\"] = \"\"\nDICTIONARY[\"B\"] = \"\"\nDICTIONARY[\"admissions\"] = \"\"\n\nprint(\"Total train data\", len(DICTIONARY))","metadata":{"id":"Hewa9fMXGz1a","outputId":"b2fe326e-374a-4bd6-97dc-39ec45fd85a8","execution":{"iopub.status.busy":"2023-10-30T12:30:12.387065Z","iopub.execute_input":"2023-10-30T12:30:12.387926Z","iopub.status.idle":"2023-10-30T12:30:14.915031Z","shell.execute_reply.started":"2023-10-30T12:30:12.387893Z","shell.execute_reply":"2023-10-30T12:30:14.914116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting IPA transcription of uncommon words","metadata":{"id":"CUxIKVArGz1c"}},{"cell_type":"code","source":"import pandas as pd\n\nurl_test = \"/kaggle/input/dataverse_2023/testData.csv\"\ndf_test = pd.read_csv(url_test)\nprint(\"Shape:\", df_test.shape)\ndf_test.head(2)","metadata":{"id":"E81dAgMkKzIX","outputId":"471ce64b-c641-4a07-ce00-0b333119779d","execution":{"iopub.status.busy":"2023-10-30T12:30:22.960326Z","iopub.execute_input":"2023-10-30T12:30:22.961122Z","iopub.status.idle":"2023-10-30T12:30:23.138968Z","shell.execute_reply.started":"2023-10-30T12:30:22.961084Z","shell.execute_reply":"2023-10-30T12:30:23.138049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in df_test.iterrows():\n  row_id = row['row_id_column_name']\n  text = row['text']\n  texts = text.split()\n\n  for word in texts:\n    if word in DICTIONARY.keys():\n      continue\n\n    normalized_word = foreign_character_normalization(word)\n    state, tokens = aligned_stateful_tokenizer(normalized_word)\n\n    if len(normalized_word) == 0:\n      DICTIONARY[word] = \"\"\n      continue\n\n    for i in range(len(state)):\n      if state[i]:\n        tokenized_word = tokens[i]\n        translated = decode_sequence(word_sentence(tokenized_word))\n        trg = sentence_word(translated)\n        trg = trg[7:]\n        trg = trg[:-5]\n        tokens[i] = trg\n\n    value = \"\".join(tokens)\n    DICTIONARY[word] = value\n\n  print(\"----->\", index)","metadata":{"id":"krMsZawBGz1c","outputId":"b3218098-46fb-4f77-b0e1-bedf7c0cbfdc","execution":{"iopub.status.busy":"2023-10-30T12:30:27.237070Z","iopub.execute_input":"2023-10-30T12:30:27.237449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(DICTIONARY))","metadata":{"id":"JZfUNyfyGz1d","outputId":"e2c1695f-cae5-4d35-c592-ffccbc28b726","execution":{"iopub.status.busy":"2023-10-30T13:34:46.357296Z","iopub.execute_input":"2023-10-30T13:34:46.357675Z","iopub.status.idle":"2023-10-30T13:34:46.362822Z","shell.execute_reply.started":"2023-10-30T13:34:46.357639Z","shell.execute_reply":"2023-10-30T13:34:46.361885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post processing","metadata":{"id":"CeXc2SATGz1e"}},{"cell_type":"code","source":"import pandas as pd\n\nBANGLA_DIGIT = ['১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯', '০']\n\ndef load_dic(DICTIONARY):\n  dic = {}\n  for key in DICTIONARY:\n    word = key\n    ipa = DICTIONARY[key]\n    if not(type(ipa) == type('cat')):\n      ipa = word\n\n    # Eleminiting bangla digits\n    ipa_ = \"\"\n    for ch in ipa:\n      if ch not in BANGLA_DIGIT:\n        ipa_ += ch\n    dic[word] = ipa_\n\n  print(\"Dictionary Loaded...\")\n  return dic\n\ndic = load_dic(DICTIONARY)","metadata":{"id":"5q0fPxVPGz1e","outputId":"0ca427fe-2376-4c8e-f697-533844d7de34","execution":{"iopub.status.busy":"2023-10-30T13:36:47.963192Z","iopub.execute_input":"2023-10-30T13:36:47.963941Z","iopub.status.idle":"2023-10-30T13:36:48.241958Z","shell.execute_reply.started":"2023-10-30T13:36:47.963909Z","shell.execute_reply":"2023-10-30T13:36:48.240869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Test set and generate submission\n","metadata":{"id":"5rkT0U3LGz1f"}},{"cell_type":"code","source":"def generate_submission(dic):\n  url_test = \"/kaggle/input/dataverse_2023/testData.csv\"\n  df_test = pd.read_csv(url_test)\n  rows = []\n  ipas = []\n\n  for index, row in df_test.iterrows():\n    row_id = row['row_id_column_name']\n    text = row['text']\n    texts = text.split()\n    pred = []\n\n    for word in texts:\n      ipa = dic[word]\n      pred.append(ipa)\n\n    ipa_text = \" \".join(pred)\n    rows.append(row_id)\n    ipas.append(ipa_text)\n\n  return rows, ipas\nrows, ipas = generate_submission(dic)\nprint(len(rows), len(ipas))","metadata":{"id":"jNEHOyJ4Gz1f","outputId":"1f261c02-a5f1-4d1f-94d9-7b6719a28dec","execution":{"iopub.status.busy":"2023-10-30T13:37:16.425203Z","iopub.execute_input":"2023-10-30T13:37:16.425915Z","iopub.status.idle":"2023-10-30T13:37:18.476015Z","shell.execute_reply.started":"2023-10-30T13:37:16.425881Z","shell.execute_reply":"2023-10-30T13:37:18.474932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving submission file","metadata":{}},{"cell_type":"code","source":"def submission_file(rows, ipas):\n  data = {\n    'row_id_column_name': rows,\n    'ipa': ipas\n  }\n\n  df = pd.DataFrame(data, columns=data.keys())\n  df.to_csv('/kaggle/working/submission15_group.csv', index=False)\nsubmission_file(rows, ipas)","metadata":{"id":"KcszQ9R2Gz1g","execution":{"iopub.status.busy":"2023-10-30T13:38:11.234575Z","iopub.execute_input":"2023-10-30T13:38:11.235440Z","iopub.status.idle":"2023-10-30T13:38:11.441328Z","shell.execute_reply.started":"2023-10-30T13:38:11.235404Z","shell.execute_reply":"2023-10-30T13:38:11.440329Z"},"trusted":true},"execution_count":null,"outputs":[]}]}